_target_: src.models.proposed_synthesis_module.ProposedSynthesisModule

name: ProposedSynthesis

optimizer:
  _target_: torch.optim.Adam
  _partial_: true
  lr: 0.0001
  betas: [0.5, 0.999]
  weight_decay: 0.0001

scheduler:
  _target_: torch.optim.lr_scheduler.MultiStepLR
  _partial_: true
  milestones: [250000, 300000]
  gamma: 0.5

netG_A:
  _target_: src.models.components.networks_define.define_G
  netG_type: 'proposed_synthesis'
  input_nc: 1
  output_nc: 1
  feat_ch: 256
  demodulate: true
  # init_type: 'kaiming'

netD_A:
  _target_: src.models.components.networks_define.define_D
  input_nc: 1
  ndf: 64
  norm: 'instance'
  n_layers_D: 3
  init_type: 'normal' #'kaiming'

netF_A: # For PatchNCELoss
  _target_: src.models.components.networks_define.define_F
  netF_type: 'mlp_sample'
  use_mlp: True #True
  init_type: 'xavier' #'xavier'
  init_gain: 0.02
  nc: 256
  input_nc: ${model.netG_A.feat_ch}



params: # Other params
  lambda_style: 5
  # lambda_cycle_a: 1
  # lambda_cycle_b: 1
  # lambda_sc: 0.1
  lambda_nce: 5
  reverse: ${data.reverse} # A->B if False, B->A if True
  use_split_inference: ${data.use_split_inference}
  flip_equivariance: False
  batch_size: ${data.batch_size}
  nce_layers: [0,2,4,6] # [0,2,4,6] # 3까지가 encoder # [0,1,2,3]-> encoder만 하니까 성능 떨어진다. 전부다 하는게 낫다.
  eval_on_align: ${data.eval_on_align}
  use_sliding_inference: ${data.use_sliding_inference}
  use_multicontrast: ${data.use_multicontrast} # Set True if data_group_3 is exist